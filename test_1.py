import streamlit as st
import sounddevice as sd
import vosk
import queue
import json
import numpy as np
import time
from deep_translator import GoogleTranslator
from gtts import gTTS
from playsound import playsound
import tempfile
import threading

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
MODEL_PATH = "vosk-model-small-ru-0.22/vosk-model-small-ru-0.22"
SAMPLE_RATE = 16000  # –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
BATCH_TIME = 4  # –í—Ä–µ–º—è (–≤ —Å–µ–∫.), —á–µ—Ä–µ–∑ –∫–æ—Ç–æ—Ä–æ–µ —Ç–µ–∫—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
SPEECH_QUEUE = queue.Queue()  # –û—á–µ—Ä–µ–¥—å –¥–ª—è –æ–∑–≤—É—á–∫–∏

col1, col2, col3 = st.columns([1, 6, 1])  # –ü—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–æ–ª–æ–Ω–æ–∫

with col2:  # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç –≤–æ –≤—Ç–æ—Ä–æ–π –∫–æ–ª–æ–Ω–∫–µ
    # –ú–µ–Ω—å—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    st.image("resize_photo_2025-02-16_12-40-54.jpg", width=400)  # –£–∫–∞–∑–∞–Ω —Ä–∞–∑–º–µ—Ä —à–∏—Ä–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è


# === –û—á–µ—Ä–µ–¥—å –¥–ª—è –∞—É–¥–∏–æ ===
q = queue.Queue()

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞ ===
def audio_callback(indata, frames, time, status):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞—É–¥–∏–æ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ –æ—á–µ—Ä–µ–¥—å"""
    if status:
        st.error(f"–û—à–∏–±–∫–∞ –∞—É–¥–∏–æ: {status}")
    try:
        if np.all(indata == 0):
            return  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç–æ–π –∏–ª–∏ —Ç–∏—Ö–∏–π —Ñ—Ä–µ–π–º
        q.put(bytes(indata))
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∞—É–¥–∏–æ –≤ –æ—á–µ—Ä–µ–¥—å: {e}")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Vosk ===
model = vosk.Model(MODEL_PATH)
recognizer = vosk.KaldiRecognizer(model, SAMPLE_RATE)

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit ===
st.title("Resonance")
st.text("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞.")

# === –í—ã–±–æ—Ä —è–∑—ã–∫–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ ===
language_codes = {
    "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π": "en",
    "–ù–µ–º–µ—Ü–∫–∏–π": "de",
    "–§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π": "fr",
    "–ò—Å–ø–∞–Ω—Å–∫–∏–π": "es",
    "–ö–∏—Ç–∞–π—Å–∫–∏–π": "zh-TW",
    "–Ø–ø–æ–Ω—Å–∫–∏–π": "ja",
    "–ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π": "pt",
    "–•–∏–Ω–¥–∏": "hi",
    "–ê—Ä–∞–±—Å–∫–∏–π": "ar"
}
target_language = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:", list(language_codes.keys()))

# === –í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ –æ–∑–≤—É—á–∫–∏ ===
enable_speech = st.checkbox("üîä –í–∫–ª—é—á–∏—Ç—å –æ–∑–≤—É—á–∫—É –ø–µ—Ä–µ–≤–æ–¥–∞", value=True)

# === –ü–æ–ª—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ ===
subtitle_text = st.empty()  # –¢–µ–∫—É—â–∏–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç
translated_text = st.empty()  # –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
history_text = st.empty()   # –ò—Å—Ç–æ—Ä–∏—è –≤—Å–µ—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑

# –•—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
if "history" not in st.session_state:
    st.session_state.history = ""
if "translated_history" not in st.session_state:
    st.session_state.translated_history = ""

# === –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ ===
def translate_text(text, target_lang):
    try:
        translation = GoogleTranslator(source="auto", target=language_codes[target_lang]).translate(text)
        return translation
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return ""

# === –§—É–Ω–∫—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ (–æ–∑–≤—É—á–∫–∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–≤–æ–¥–∞) ===
def speak_text():
    """–û–∑–≤—É—á–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É"""
    while True:
        text, lang = SPEECH_QUEUE.get()  # –ë–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        if text.strip():
            try:
                with tempfile.NamedTemporaryFile(delete=True, suffix=".mp3") as temp_audio:
                    tts = gTTS(text=text, lang=lang)
                    tts.save(temp_audio.name)
                    playsound(temp_audio.name)  # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –∑–≤—É–∫
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {e}")
        SPEECH_QUEUE.task_done()  # –ü–æ–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –¥–ª—è –æ–∑–≤—É—á–∫–∏
speech_thread = threading.Thread(target=speak_text, daemon=True)
speech_thread.start()

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ ===
def process_audio():
    with sd.RawInputStream(samplerate=SAMPLE_RATE, blocksize=8000, dtype="int16",
                           channels=1, callback=audio_callback):
        st.write("üé§ –ì–æ–≤–æ—Ä–∏—Ç–µ...")
        start_time = time.time()
        
        while True:
            data = q.get()
            if recognizer.AcceptWaveform(data) or (time.time() - start_time > BATCH_TIME):
                result = json.loads(recognizer.Result())
                start_time = time.time()  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
                
                if result["text"].strip():
                    time.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–æ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    recognized_text = result["text"]
                    subtitle_text.text(f"üìù –¢–µ–∫—É—â–∏–µ —Å–ª–æ–≤–∞: {recognized_text}")

                    # –ü–µ—Ä–µ–≤–æ–¥
                    translated = translate_text(recognized_text, target_language)
                    translated_text.text(f"üåç –ü–µ—Ä–µ–≤–æ–¥: {translated}")

                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                    st.session_state.history += recognized_text + " "
                    st.session_state.translated_history += translated + " "

                    history_text.text(f"üìú –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{st.session_state.history}\n\n"
                                      f"üåç –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥:\n{st.session_state.translated_history}")

                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –æ–∑–≤—É—á–∫—É
                    if enable_speech:
                        SPEECH_QUEUE.put((translated, language_codes[target_language]))

            else:
                partial_result = recognizer.PartialResult()
                partial_data = json.loads(partial_result)
                if partial_data.get("partial", "").strip():
                    subtitle_text.text(f"üìù –¢–µ–∫—É—â–∏–µ —Å–ª–æ–≤–∞: {partial_data['partial']}")

# === –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ===
col1, col2 = st.columns(2)
with col1:
    if st.button("üéôÔ∏è –ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"):
        process_audio()
with col2:
    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
        st.session_state.history = ""
        st.session_state.translated_history = ""
        history_text.text("üìú –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞.")